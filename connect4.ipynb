{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:54:54.389724Z",
     "start_time": "2025-03-27T16:54:54.378449Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Connect4Env:\n",
    "    def __init__(self):\n",
    "        self.rows = 6\n",
    "        self.cols = 7\n",
    "        self.board = np.zeros((self.rows, self.cols), dtype=int)\n",
    "        self.current_player = 1  # Player 1 = 1, Player 2 = -1\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset b√†n c·ªù\"\"\"\n",
    "        self.board = np.zeros((self.rows, self.cols), dtype=int)\n",
    "        self.current_player = 1\n",
    "        return self.board.flatten()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Th·ª±c hi·ªán m·ªôt h√†nh ƒë·ªông\"\"\"\n",
    "        if action not in self.valid_moves():\n",
    "            return self.board.flatten(), -10, True, {}  # K·∫øt th√∫c n·∫øu ch·ªçn sai\n",
    "\n",
    "        for row in reversed(range(self.rows)):  # T√¨m h√†ng tr·ªëng th·∫•p nh·∫•t\n",
    "            if self.board[row, action] == 0:\n",
    "                self.board[row, action] = self.current_player\n",
    "                reward, done = self.check_winner()\n",
    "                self.current_player *= -1  # ƒê·ªïi l∆∞·ª£t\n",
    "                return self.board.flatten(), reward, done, {}\n",
    "\n",
    "        return self.board.flatten(), -10, True, {}  # K·∫øt th√∫c n·∫øu c·ªôt ƒë·∫ßy\n",
    "\n",
    "    def check_winner(self):\n",
    "        \"\"\"Ki·ªÉm tra xem c√≥ ai th·∫Øng kh√¥ng\"\"\"\n",
    "    # Ki·ªÉm tra h√†ng ngang\n",
    "        for row in range(self.rows):\n",
    "            for col in range(self.cols - 3):\n",
    "                if abs(sum(self.board[row, col:col+4])) == 4:\n",
    "                    return (10, True)\n",
    "\n",
    "    # Ki·ªÉm tra c·ªôt d·ªçc\n",
    "        for row in range(self.rows - 3):\n",
    "            for col in range(self.cols):\n",
    "                if abs(sum(self.board[row:row+4, col])) == 4:\n",
    "                    return (10, True)\n",
    "\n",
    "    # Ki·ªÉm tra ƒë∆∞·ªùng ch√©o ch√≠nh (\\)\n",
    "        for row in range(self.rows - 3):\n",
    "            for col in range(self.cols - 3):\n",
    "                diag = [self.board[row+i, col+i] for i in range(4)]\n",
    "                if abs(sum(diag)) == 4:\n",
    "                    return (10, True)\n",
    "\n",
    "    # Ki·ªÉm tra ƒë∆∞·ªùng ch√©o ph·ª• (/)\n",
    "        for row in range(self.rows - 3):\n",
    "            for col in range(3, self.cols):\n",
    "                diag = [self.board[row+i, col-i] for i in range(4)]\n",
    "                if abs(sum(diag)) == 4:\n",
    "                    return (10, True)\n",
    "\n",
    "    # Ki·ªÉm tra h√≤a\n",
    "        if np.all(self.board != 0):\n",
    "            return (0, True)\n",
    "\n",
    "        return (0, False)  # Game ch∆∞a k·∫øt th√∫c\n",
    "\n",
    "    def valid_moves(self):\n",
    "        \"\"\"Tr·∫£ v·ªÅ danh s√°ch c·ªôt c√≥ th·ªÉ ƒëi\"\"\"\n",
    "        return [c for c in range(self.cols) if self.board[0, c] == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:55:03.471227Z",
     "start_time": "2025-03-27T16:55:03.452719Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)  # Kh√¥ng d√πng softmax v√¨ DQN t·ªëi ∆∞u gi√° tr·ªã Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-27T16:55:06.430943Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_17960\\3747478758.py:17: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# epilson decay graph\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.1\n",
    "\n",
    "max_episode = 50\n",
    "episode = np.arange(max_episode)\n",
    "epsilon_decay = np.log(epsilon_start/epsilon_end*100) / max_episode\n",
    "\n",
    "eps = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-episode * epsilon_decay)\n",
    "plt.plot(episode, eps)\n",
    "plt.title('Epsilon decay graph')\n",
    "plt.xlabel('Episode no.')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T12:37:35.326498Z",
     "iopub.status.busy": "2025-03-26T12:37:35.326186Z",
     "iopub.status.idle": "2025-03-26T12:37:35.331283Z",
     "shell.execute_reply": "2025-03-26T12:37:35.330424Z",
     "shell.execute_reply.started": "2025-03-26T12:37:35.326471Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_action(model, state, episode=None, training=True):\n",
    "    \"\"\"Select action using epsilon-greedy with decay.\"\"\"\n",
    "    # Calculate epsilon based on episode if training, otherwise no exploration\n",
    "    epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-episode * epsilon_decay) if training else 0.0\n",
    "\n",
    "    if random.random() < epsilon:\n",
    "        return np.random.choice(range(7))\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = model(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
    "            return torch.argmax(q_values).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T12:37:37.366285Z",
     "iopub.status.busy": "2025-03-26T12:37:37.365970Z",
     "iopub.status.idle": "2025-03-26T12:37:37.370840Z",
     "shell.execute_reply": "2025-03-26T12:37:37.370188Z",
     "shell.execute_reply.started": "2025-03-26T12:37:37.366259Z"
    }
   },
   "outputs": [],
   "source": [
    "def play_game(dqn_player1, dqn_player2, env, epsilon):\n",
    "    \"\"\"ƒê·ªÉ hai model t·ª± ch∆°i v·ªõi nhau\"\"\"\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    turn = 0  # 0 l√† Player 1, 1 l√† Player 2\n",
    "\n",
    "    while not done:\n",
    "        model = dqn_player1 if turn == 0 else dqn_player2\n",
    "        action = select_action(model, state, epsilon)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            return reward  # Tr·∫£ v·ªÅ k·∫øt qu·∫£ tr·∫≠n ƒë·∫•u\n",
    "\n",
    "        state = next_state\n",
    "        turn = 1 - turn  # ƒê·ªïi l∆∞·ª£t ch∆°i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T12:37:39.454302Z",
     "iopub.status.busy": "2025-03-26T12:37:39.454026Z",
     "iopub.status.idle": "2025-03-26T12:37:39.459829Z",
     "shell.execute_reply": "2025-03-26T12:37:39.459003Z",
     "shell.execute_reply.started": "2025-03-26T12:37:39.454281Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def train_model(model, optimizer, memory, batch_size):\n",
    "    \"\"\"Hu·∫•n luy·ªán model DQN\"\"\"\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    \n",
    "    for state, action, reward, next_state, done in batch:\n",
    "        q_values = model(torch.tensor(state, dtype=torch.float32))\n",
    "        q_value = q_values[action]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_q_values = model(torch.tensor(next_state, dtype=torch.float32))\n",
    "            target_q_value = reward if done else reward + 0.99 * torch.max(next_q_values)\n",
    "\n",
    "        loss = (q_value - target_q_value) ** 2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T12:37:43.459024Z",
     "iopub.status.busy": "2025-03-26T12:37:43.458714Z",
     "iopub.status.idle": "2025-03-26T12:39:18.453038Z",
     "shell.execute_reply": "2025-03-26T12:39:18.452234Z",
     "shell.execute_reply.started": "2025-03-26T12:37:43.459001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Reward 10\n",
      "Episode 10: Reward 10\n",
      "Episode 20: Reward 10\n",
      "Episode 30: Reward 10\n",
      "Episode 40: Reward 10\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "env = Connect4Env()\n",
    "\n",
    "input_dim = 6 * 7  # B√†n c·ªù 6x7\n",
    "output_dim = 7  # 7 c·ªôt\n",
    "\n",
    "dqn_player1 = DQN(input_dim, output_dim)\n",
    "dqn_player2 = DQN(input_dim, output_dim)\n",
    "\n",
    "optimizer1 = optim.Adam(dqn_player1.parameters(), lr=0.001)\n",
    "optimizer2 = optim.Adam(dqn_player2.parameters(), lr=0.001)\n",
    "\n",
    "memory = deque(maxlen=10000)\n",
    "batch_size = 64\n",
    "num_episodes = 50\n",
    "epsilon = 0.1\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    turn = 0  # 0 l√† Player 1, 1 l√† Player 2\n",
    "\n",
    "    while not done:\n",
    "        model = dqn_player1 if turn == 0 else dqn_player2\n",
    "        optimizer = optimizer1 if turn == 0 else optimizer2\n",
    "\n",
    "        action = select_action(model, state, episode)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        # Hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "        train_model(model, optimizer, memory, batch_size)\n",
    "\n",
    "        state = next_state\n",
    "        turn = 1 - turn  # ƒê·ªïi l∆∞·ª£t ch∆°i\n",
    "\n",
    "    epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-episode * epsilon_decay)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}: Reward {reward}, Epsilon {epsilon:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T12:41:01.745381Z",
     "iopub.status.busy": "2025-03-26T12:41:01.745106Z",
     "iopub.status.idle": "2025-03-26T12:41:01.767252Z",
     "shell.execute_reply": "2025-03-26T12:41:01.766416Z",
     "shell.execute_reply.started": "2025-03-26T12:41:01.745361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Th·ªùi gian ho√†n th√†nh 1 game: 0.02 gi√¢y\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "env.reset()\n",
    "start_time = time.time()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action = select_action(dqn_player1, env.board.flatten(), training=False)\n",
    "    _, _, done, _ = env.step(action)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Th·ªùi gian ho√†n th√†nh 1 game: {end_time - start_time:.2f} gi√¢y\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T12:41:07.912414Z",
     "iopub.status.busy": "2025-03-26T12:41:07.912096Z",
     "iopub.status.idle": "2025-03-26T12:41:08.041336Z",
     "shell.execute_reply": "2025-03-26T12:41:08.040520Z",
     "shell.execute_reply.started": "2025-03-26T12:41:07.912388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Model's Average Reward over 10 games: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_model(model, env, num_games=10):\n",
    "    total_rewards = 0\n",
    "    for _ in range(num_games):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        move_count = 0\n",
    "        while not done:\n",
    "            action = select_action(model, state, training=False)  # Lu√¥n ch·ªçn h√†nh ƒë·ªông t·ªët nh·∫•t\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            # print(f\"üßê Ch·ªçn c·ªôt: {action}\")\n",
    "            move_count +=1\n",
    "            if move_count > 42:\n",
    "                print(\"qua 42 buoc\")\n",
    "                print(\"Board state:\\n\", env.board)\n",
    "                break\n",
    "        \n",
    "        total_rewards += reward  # C·ªông t·ªïng ƒëi·ªÉm th∆∞·ªüng\n",
    "        \n",
    "    avg_reward = total_rewards / num_games\n",
    "    print(f\"üìä Model's Average Reward over {num_games} games: {avg_reward}\")\n",
    "    return avg_reward\n",
    "\n",
    "# ƒê√°nh gi√° model sau khi train\n",
    "evaluate_model(dqn_player1, env, num_games=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T12:42:51.359909Z",
     "iopub.status.busy": "2025-03-26T12:42:51.359616Z",
     "iopub.status.idle": "2025-03-26T12:42:51.366453Z",
     "shell.execute_reply": "2025-03-26T12:42:51.365791Z",
     "shell.execute_reply.started": "2025-03-26T12:42:51.359887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "torch.save(dqn_player1.state_dict(), \"dqn_player1.pth\")\n",
    "torch.save(dqn_player2.state_dict(), \"dqn_player2.pth\")\n",
    "print(\"‚úÖ Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-26T12:24:43.197873Z",
     "iopub.status.idle": "2025-03-26T12:24:43.198119Z",
     "shell.execute_reply": "2025-03-26T12:24:43.198022Z"
    }
   },
   "outputs": [],
   "source": [
    "dqn_player1.load_state_dict(torch.load(\"dqn_player1.pth\"))\n",
    "dqn_player2.load_state_dict(torch.load(\"dqn_player2.pth\"))\n",
    "print(\"‚úÖ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T12:32:18.422647Z",
     "iopub.status.busy": "2025-03-26T12:32:18.422332Z",
     "iopub.status.idle": "2025-03-26T12:32:18.762955Z",
     "shell.execute_reply": "2025-03-26T12:32:18.762200Z",
     "shell.execute_reply.started": "2025-03-26T12:32:18.422618Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Model's Average Reward over 100 games: -10.0\n",
      "‚è≥ Time taken: 0.34 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "evaluate_model(dqn_player1, env, num_games=100)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"‚è≥ Time taken: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T12:32:25.381134Z",
     "iopub.status.busy": "2025-03-26T12:32:25.380859Z",
     "iopub.status.idle": "2025-03-26T12:32:25.386534Z",
     "shell.execute_reply": "2025-03-26T12:32:25.385755Z",
     "shell.execute_reply.started": "2025-03-26T12:32:25.381113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Th·ªùi gian ch·ªçn action: 0.0014 gi√¢y\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "action = select_action(model, state, epsilon=0.0)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"‚è≥ Th·ªùi gian ch·ªçn action: {end - start:.4f} gi√¢y\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
