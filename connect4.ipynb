{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport random\n\nclass Connect4Env:\n    def __init__(self):\n        self.rows = 6\n        self.cols = 7\n        self.board = np.zeros((self.rows, self.cols), dtype=int)\n        self.current_player = 1  # Player 1 = 1, Player 2 = -1\n\n    def reset(self):\n        \"\"\"Reset b√†n c·ªù\"\"\"\n        self.board = np.zeros((self.rows, self.cols), dtype=int)\n        self.current_player = 1\n        return self.board.flatten()\n\n    def step(self, action):\n        \"\"\"Th·ª±c hi·ªán m·ªôt h√†nh ƒë·ªông\"\"\"\n        if action not in self.valid_moves():\n            return self.board.flatten(), -10, True, {}  # K·∫øt th√∫c n·∫øu ch·ªçn sai\n\n        for row in reversed(range(self.rows)):  # T√¨m h√†ng tr·ªëng th·∫•p nh·∫•t\n            if self.board[row, action] == 0:\n                self.board[row, action] = self.current_player\n                reward, done = self.check_winner()\n                self.current_player *= -1  # ƒê·ªïi l∆∞·ª£t\n                return self.board.flatten(), reward, done, {}\n\n        return self.board.flatten(), -10, True, {}  # K·∫øt th√∫c n·∫øu c·ªôt ƒë·∫ßy\n\n    def check_winner(self):\n        \"\"\"Ki·ªÉm tra xem c√≥ ai th·∫Øng kh√¥ng\"\"\"\n    # Ki·ªÉm tra h√†ng ngang\n        for row in range(self.rows):\n            for col in range(self.cols - 3):\n                if abs(sum(self.board[row, col:col+4])) == 4:\n                    return (10, True)\n\n    # Ki·ªÉm tra c·ªôt d·ªçc\n        for row in range(self.rows - 3):\n            for col in range(self.cols):\n                if abs(sum(self.board[row:row+4, col])) == 4:\n                    return (10, True)\n\n    # Ki·ªÉm tra ƒë∆∞·ªùng ch√©o ch√≠nh (\\)\n        for row in range(self.rows - 3):\n            for col in range(self.cols - 3):\n                diag = [self.board[row+i, col+i] for i in range(4)]\n                if abs(sum(diag)) == 4:\n                    return (10, True)\n\n    # Ki·ªÉm tra ƒë∆∞·ªùng ch√©o ph·ª• (/)\n        for row in range(self.rows - 3):\n            for col in range(3, self.cols):\n                diag = [self.board[row+i, col-i] for i in range(4)]\n                if abs(sum(diag)) == 4:\n                    return (10, True)\n\n    # Ki·ªÉm tra h√≤a\n        if np.all(self.board != 0):\n            return (0, True)\n\n        return (0, False)  # Game ch∆∞a k·∫øt th√∫c\n\n    def valid_moves(self):\n        \"\"\"Tr·∫£ v·ªÅ danh s√°ch c·ªôt c√≥ th·ªÉ ƒëi\"\"\"\n        return [c for c in range(self.cols) if self.board[0, c] == 0]\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T12:30:37.350105Z","iopub.execute_input":"2025-03-26T12:30:37.350387Z","iopub.status.idle":"2025-03-26T12:30:37.360305Z","shell.execute_reply.started":"2025-03-26T12:30:37.350365Z","shell.execute_reply":"2025-03-26T12:30:37.359322Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass DQN(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(DQN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, output_dim)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        return self.fc3(x)  # Kh√¥ng d√πng softmax v√¨ DQN t·ªëi ∆∞u gi√° tr·ªã Q\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T12:30:39.835427Z","iopub.execute_input":"2025-03-26T12:30:39.835719Z","iopub.status.idle":"2025-03-26T12:30:39.840841Z","shell.execute_reply.started":"2025-03-26T12:30:39.835698Z","shell.execute_reply":"2025-03-26T12:30:39.840075Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def select_action(model, state, epsilon):\n    \"\"\"Ch·ªçn h√†nh ƒë·ªông b·∫±ng epsilon-greedy v·ªõi ki·ªÉm tra h·ª£p l·ªá\"\"\"\n    valid_moves = env.valid_moves()  # L·∫•y danh s√°ch c√°c c·ªôt h·ª£p l·ªá\n\n    if random.random() < epsilon:\n        return np.random.choice(valid_moves)  # Ch·ªçn ng·∫´u nhi√™n trong c√°c c·ªôt h·ª£p l·ªá\n    else:\n        with torch.no_grad():\n            q_values = model(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n            q_values = q_values.numpy().flatten()  # Chuy·ªÉn th√†nh m·∫£ng numpy\n\n        # Ch·ªâ ch·ªçn h√†nh ƒë·ªông c√≥ Q-value cao nh·∫•t trong c√°c c·ªôt h·ª£p l·ªá\n        best_action = max(valid_moves, key=lambda a: q_values[a])\n        return best_action\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T12:37:35.326186Z","iopub.execute_input":"2025-03-26T12:37:35.326498Z","iopub.status.idle":"2025-03-26T12:37:35.331283Z","shell.execute_reply.started":"2025-03-26T12:37:35.326471Z","shell.execute_reply":"2025-03-26T12:37:35.330424Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def play_game(dqn_player1, dqn_player2, env, epsilon):\n    \"\"\"ƒê·ªÉ hai model t·ª± ch∆°i v·ªõi nhau\"\"\"\n    state = env.reset()\n    done = False\n    turn = 0  # 0 l√† Player 1, 1 l√† Player 2\n\n    while not done:\n        model = dqn_player1 if turn == 0 else dqn_player2\n        action = select_action(model, state, epsilon)\n        \n        next_state, reward, done, _ = env.step(action)\n\n        if done:\n            return reward  # Tr·∫£ v·ªÅ k·∫øt qu·∫£ tr·∫≠n ƒë·∫•u\n\n        state = next_state\n        turn = 1 - turn  # ƒê·ªïi l∆∞·ª£t ch∆°i\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T12:37:37.365970Z","iopub.execute_input":"2025-03-26T12:37:37.366285Z","iopub.status.idle":"2025-03-26T12:37:37.370840Z","shell.execute_reply.started":"2025-03-26T12:37:37.366259Z","shell.execute_reply":"2025-03-26T12:37:37.370188Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"from collections import deque\n\ndef train_model(model, optimizer, memory, batch_size):\n    \"\"\"Hu·∫•n luy·ªán model DQN\"\"\"\n    if len(memory) < batch_size:\n        return\n\n    batch = random.sample(memory, batch_size)\n    \n    for state, action, reward, next_state, done in batch:\n        q_values = model(torch.tensor(state, dtype=torch.float32))\n        q_value = q_values[action]\n\n        with torch.no_grad():\n            next_q_values = model(torch.tensor(next_state, dtype=torch.float32))\n            target_q_value = reward if done else reward + 0.99 * torch.max(next_q_values)\n\n        loss = (q_value - target_q_value) ** 2\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T12:37:39.454026Z","iopub.execute_input":"2025-03-26T12:37:39.454302Z","iopub.status.idle":"2025-03-26T12:37:39.459829Z","shell.execute_reply.started":"2025-03-26T12:37:39.454281Z","shell.execute_reply":"2025-03-26T12:37:39.459003Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"env = Connect4Env()\n\ninput_dim = 6 * 7  # B√†n c·ªù 6x7\noutput_dim = 7  # 7 c·ªôt\n\ndqn_player1 = DQN(input_dim, output_dim)\ndqn_player2 = DQN(input_dim, output_dim)\n\noptimizer1 = optim.Adam(dqn_player1.parameters(), lr=0.001)\noptimizer2 = optim.Adam(dqn_player2.parameters(), lr=0.001)\n\nmemory = deque(maxlen=10000)\nbatch_size = 64\nnum_episodes = 50\nepsilon = 0.1\n\nfor episode in range(num_episodes):\n    state = env.reset()\n    done = False\n    turn = 0  # 0 l√† Player 1, 1 l√† Player 2\n\n    while not done:\n        model = dqn_player1 if turn == 0 else dqn_player2\n        optimizer = optimizer1 if turn == 0 else optimizer2\n\n        action = select_action(model, state, epsilon)\n        next_state, reward, done, _ = env.step(action)\n\n        memory.append((state, action, reward, next_state, done))\n\n        # Hu·∫•n luy·ªán m√¥ h√¨nh\n        train_model(model, optimizer, memory, batch_size)\n\n        state = next_state\n        turn = 1 - turn  # ƒê·ªïi l∆∞·ª£t ch∆°i\n\n    if episode % 10 == 0:\n        print(f\"Episode {episode}: Reward {reward}\")\n\n\nprint(\"Training complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T12:37:43.458714Z","iopub.execute_input":"2025-03-26T12:37:43.459024Z","iopub.status.idle":"2025-03-26T12:39:18.453038Z","shell.execute_reply.started":"2025-03-26T12:37:43.459001Z","shell.execute_reply":"2025-03-26T12:39:18.452234Z"}},"outputs":[{"name":"stdout","text":"Episode 0: Reward 10\nEpisode 10: Reward 10\nEpisode 20: Reward 10\nEpisode 30: Reward 10\nEpisode 40: Reward 10\nTraining complete!\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"import time\n\nenv.reset()\nstart_time = time.time()\n\ndone = False\nwhile not done:\n    action = select_action(dqn_player1, env.board.flatten(), epsilon=0.0)\n    _, _, done, _ = env.step(action)\n\nend_time = time.time()\nprint(f\"Th·ªùi gian ho√†n th√†nh 1 game: {end_time - start_time:.2f} gi√¢y\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T12:41:01.745106Z","iopub.execute_input":"2025-03-26T12:41:01.745381Z","iopub.status.idle":"2025-03-26T12:41:01.767252Z","shell.execute_reply.started":"2025-03-26T12:41:01.745361Z","shell.execute_reply":"2025-03-26T12:41:01.766416Z"}},"outputs":[{"name":"stdout","text":"Th·ªùi gian ho√†n th√†nh 1 game: 0.02 gi√¢y\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"def evaluate_model(model, env, num_games=10):\n    total_rewards = 0\n    for _ in range(num_games):\n        state = env.reset()\n        done = False\n        move_count = 0\n        while not done:\n            action = select_action(model, state, epsilon=0.0)  # Lu√¥n ch·ªçn h√†nh ƒë·ªông t·ªët nh·∫•t\n            state, reward, done, _ = env.step(action)\n            # print(f\"üßê Ch·ªçn c·ªôt: {action}\")\n            move_count +=1\n            if move_count > 42:\n                print(\"qua 42 buoc\")\n                print(\"Board state:\\n\", env.board)\n                break\n        \n        total_rewards += reward  # C·ªông t·ªïng ƒëi·ªÉm th∆∞·ªüng\n        \n    avg_reward = total_rewards / num_games\n    print(f\"üìä Model's Average Reward over {num_games} games: {avg_reward}\")\n    return avg_reward\n\n# ƒê√°nh gi√° model sau khi train\nevaluate_model(dqn_player1, env, num_games=10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T12:41:07.912096Z","iopub.execute_input":"2025-03-26T12:41:07.912414Z","iopub.status.idle":"2025-03-26T12:41:08.041336Z","shell.execute_reply.started":"2025-03-26T12:41:07.912388Z","shell.execute_reply":"2025-03-26T12:41:08.040520Z"}},"outputs":[{"name":"stdout","text":"üìä Model's Average Reward over 10 games: 0.0\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"0.0"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"torch.save(dqn_player1.state_dict(), \"dqn_player1.pth\")\ntorch.save(dqn_player2.state_dict(), \"dqn_player2.pth\")\nprint(\"‚úÖ Model saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T12:42:51.359616Z","iopub.execute_input":"2025-03-26T12:42:51.359909Z","iopub.status.idle":"2025-03-26T12:42:51.366453Z","shell.execute_reply.started":"2025-03-26T12:42:51.359887Z","shell.execute_reply":"2025-03-26T12:42:51.365791Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Model saved successfully!\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"dqn_player1.load_state_dict(torch.load(\"dqn_player1.pth\"))\ndqn_player2.load_state_dict(torch.load(\"dqn_player2.pth\"))\nprint(\"‚úÖ Model loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T12:24:43.197873Z","iopub.status.idle":"2025-03-26T12:24:43.198119Z","shell.execute_reply":"2025-03-26T12:24:43.198022Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\nstart_time = time.time()\nevaluate_model(dqn_player1, env, num_games=100)\nend_time = time.time()\n\nprint(f\"‚è≥ Time taken: {end_time - start_time:.2f} seconds\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T12:32:18.422332Z","iopub.execute_input":"2025-03-26T12:32:18.422647Z","iopub.status.idle":"2025-03-26T12:32:18.762955Z","shell.execute_reply.started":"2025-03-26T12:32:18.422618Z","shell.execute_reply":"2025-03-26T12:32:18.762200Z"}},"outputs":[{"name":"stdout","text":"üìä Model's Average Reward over 100 games: -10.0\n‚è≥ Time taken: 0.34 seconds\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import time\n\nstart = time.time()\naction = select_action(model, state, epsilon=0.0)\nend = time.time()\n\nprint(f\"‚è≥ Th·ªùi gian ch·ªçn action: {end - start:.4f} gi√¢y\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T12:32:25.380859Z","iopub.execute_input":"2025-03-26T12:32:25.381134Z","iopub.status.idle":"2025-03-26T12:32:25.386534Z","shell.execute_reply.started":"2025-03-26T12:32:25.381113Z","shell.execute_reply":"2025-03-26T12:32:25.385755Z"}},"outputs":[{"name":"stdout","text":"‚è≥ Th·ªùi gian ch·ªçn action: 0.0014 gi√¢y\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}