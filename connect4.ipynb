{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super global constant variables\n",
    "BOARD_ROW = 6\n",
    "BOARD_COL = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-26T12:30:37.350387Z",
     "iopub.status.busy": "2025-03-26T12:30:37.350105Z",
     "iopub.status.idle": "2025-03-26T12:30:37.360305Z",
     "shell.execute_reply": "2025-03-26T12:30:37.359322Z",
     "shell.execute_reply.started": "2025-03-26T12:30:37.350365Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Connect4Env:\n",
    "    def __init__(self):\n",
    "        self.rows = BOARD_ROW\n",
    "        self.cols = BOARD_COL\n",
    "        self.board = np.zeros((self.rows, self.cols), dtype = np.int8)\n",
    "        self.current_player = 1  # Player 1 = 1, Player 2 = 2\n",
    "        self.reward = {'win': 10, 'draw': 0, 'lose': -10}\n",
    "\n",
    "    def reset(self):\n",
    "        self.__init__()\n",
    "        return self.board.flatten()\n",
    "\n",
    "    def play(self, action):\n",
    "        \"\"\"Th·ª±c hi·ªán m·ªôt h√†nh ƒë·ªông\"\"\"\n",
    "        if action not in self.valid_moves():\n",
    "            return self.board.flatten(), -10, True, {}  # K·∫øt th√∫c n·∫øu ch·ªçn sai\n",
    "\n",
    "        for row in reversed(range(self.rows)):  # T√¨m h√†ng tr·ªëng th·∫•p nh·∫•t\n",
    "            if self.board[row, action] == 0:\n",
    "                self.board[row, action] = self.current_player\n",
    "                reward, done = self.isWinningMove()\n",
    "                self.current_player = 3 - self.current_player  # ƒê·ªïi l∆∞·ª£t\n",
    "                return self.board.flatten(), reward, done, {}\n",
    "\n",
    "        reward, done = self.isWinningMove()\n",
    "        if done:\n",
    "            # Tr·∫£ reward cho player hi·ªán t·∫°i\n",
    "            final_reward = reward if self.current_player == 1 else -reward\n",
    "        else:\n",
    "            final_reward = 0\n",
    "    \n",
    "        self.current_player = 3 - self.current_player\n",
    "        return self.board.flatten(), final_reward, done, {}\n",
    "    \n",
    "    \"\"\" Check if current state is ended after making a move\"\"\"\n",
    "    def isWinningMove(self):\n",
    "        def check_direction(r, c, dr, dc, player):\n",
    "            count = 0\n",
    "            for _ in range(4):\n",
    "                if 0 <= r < self.rows and 0 <= c < self.cols and self.board[r, c] == player:\n",
    "                    count += 1\n",
    "                    r += dr\n",
    "                    c += dc\n",
    "                else:\n",
    "                    break\n",
    "            return count == 4\n",
    "\n",
    "        for r in reversed(range(self.rows)):\n",
    "            for c in range(self.cols):\n",
    "                if self.board[r, c] != 0:\n",
    "                    player = self.board[r, c]\n",
    "                    if (check_direction(r, c, 1, 0, player) or  # Vertical\n",
    "                            check_direction(r, c, 0, 1, player) or  # Horizontal\n",
    "                            check_direction(r, c, 1, 1, player) or  # Diagonal /\n",
    "                            check_direction(r, c, 1, -1, player)):  # Diagonal \\\n",
    "                        return (self.reward['win'], True)\n",
    "                    \n",
    "        # Check draw game\n",
    "        if np.all(self.board != 0):\n",
    "            return (self.reward['draw'], True)\n",
    "        \n",
    "        return (0, False) # have not done yet\n",
    "\n",
    "    def valid_moves(self):\n",
    "        \"\"\"Tr·∫£ v·ªÅ danh s√°ch c·ªôt c√≥ th·ªÉ ƒëi\"\"\"\n",
    "        return [c for c in range(self.cols) if self.board[0, c] == 0]\n",
    "    \n",
    "    def printBoard(self):\n",
    "        for row in self.board:\n",
    "            print(\" \".join([\"‚ö´\" if x == 0 else \"üöó\" if x == 1 else \"üöï\" for x in row]))\n",
    "        print(\" 0  1   2  3   4  5   6\")\n",
    "\n",
    "board = Connect4Env()\n",
    "state = board.reset()\n",
    "print(state)\n",
    "print(board.board)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_episodes = []  \n",
    "all_win_rates = []      \n",
    "all_avg_rewards = []  \n",
    "all_avg_win_steps = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T12:30:39.835719Z",
     "iopub.status.busy": "2025-03-26T12:30:39.835427Z",
     "iopub.status.idle": "2025-03-26T12:30:39.840841Z",
     "shell.execute_reply": "2025-03-26T12:30:39.840075Z",
     "shell.execute_reply.started": "2025-03-26T12:30:39.835698Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        return self.layer3(x)  # Kh√¥ng d√πng softmax v√¨ DQN t·ªëi ∆∞u gi√° tr·ªã Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# epilson decay graph\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.1\n",
    "\n",
    "max_episode = 50\n",
    "episode = np.arange(max_episode)\n",
    "epsilon_decay = np.log(epsilon_start/epsilon_end*100) / max_episode\n",
    "\n",
    "eps = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-episode * epsilon_decay)\n",
    "plt.plot(episode, eps)\n",
    "print(epsilon_decay)\n",
    "print(eps)\n",
    "plt.title('Epsilon decay graph')\n",
    "plt.xlabel('Episode no.')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T12:37:35.326498Z",
     "iopub.status.busy": "2025-03-26T12:37:35.326186Z",
     "iopub.status.idle": "2025-03-26T12:37:35.331283Z",
     "shell.execute_reply": "2025-03-26T12:37:35.330424Z",
     "shell.execute_reply.started": "2025-03-26T12:37:35.326471Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Use Epsilon-greedy to find out the best way to make a move\n",
    "\n",
    "    @Params:\n",
    "    @Return: best column to play\"\"\"\n",
    "def select_action(model, state, valid_moves, episode=None, training=True):\n",
    "    if not valid_moves:  # N·∫øu kh√¥ng c√≥ n∆∞·ªõc h·ª£p l·ªá\n",
    "        return None\n",
    "    \n",
    "    if not training:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            q_values = model(state_tensor)\n",
    "            q_values = q_values[valid_moves]  # Ch·ªâ x√©t c√°c c·ªôt h·ª£p l·ªá\n",
    "            return valid_moves[torch.argmax(q_values).item()]\n",
    "    \n",
    "    # T√≠nh epsilon v·ªõi decay\n",
    "    epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-episode * epsilon_decay)\n",
    "    \n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(valid_moves)  # Exploration\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            q_values = model(state_tensor)\n",
    "            q_values = q_values[valid_moves]  # Mask invalid moves\n",
    "            return valid_moves[torch.argmax(q_values).item()]  # Exploitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T12:37:37.366285Z",
     "iopub.status.busy": "2025-03-26T12:37:37.365970Z",
     "iopub.status.idle": "2025-03-26T12:37:37.370840Z",
     "shell.execute_reply": "2025-03-26T12:37:37.370188Z",
     "shell.execute_reply.started": "2025-03-26T12:37:37.366259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Let 2 models play against each other\n",
    "    @Return: reward after play optimally\"\"\"\n",
    "def play_game(dqn_player1, dqn_player2, env, epsilon):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    turn = 1  # 1 l√† Player 1, 2 l√† Player 2\n",
    "\n",
    "    while not done:\n",
    "        model = dqn_player1 if turn == 1 else dqn_player2\n",
    "        action = select_action(model, state, epsilon)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            return reward  # Tr·∫£ v·ªÅ k·∫øt qu·∫£ tr·∫≠n ƒë·∫•u\n",
    "\n",
    "        state = next_state\n",
    "        turn = 3 - turn  # ƒê·ªïi l∆∞·ª£t ch∆°i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T12:37:39.454302Z",
     "iopub.status.busy": "2025-03-26T12:37:39.454026Z",
     "iopub.status.idle": "2025-03-26T12:37:39.459829Z",
     "shell.execute_reply": "2025-03-26T12:37:39.459003Z",
     "shell.execute_reply.started": "2025-03-26T12:37:39.454281Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "\"\"\"@Params: \"\"\"\n",
    "def train_model(model, target_model, optimizer, memory, batch_size):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    \n",
    "    # Chuy·ªÉn sang tensor\n",
    "    states = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.long)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "    next_states = torch.tensor(np.array(next_states), dtype=torch.float32)\n",
    "    dones = torch.tensor(dones, dtype=torch.bool)\n",
    "    \n",
    "    # T√≠nh Q values hi·ªán t·∫°i\n",
    "    current_q = model(states).gather(1, actions.unsqueeze(1))\n",
    "    \n",
    "    # T√≠nh target Q values\n",
    "    with torch.no_grad():\n",
    "        next_actions = model(next_states).max(1)[1]\n",
    "        next_q = target_model(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()\n",
    "        target_q = rewards + (1 - dones.float()) * 0.99 * next_q\n",
    "    \n",
    "    # T√≠nh loss\n",
    "    loss = nn.MSELoss()(current_q.squeeze(), target_q)\n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, env, num_games=100):\n",
    "    model.eval()\n",
    "    total_wins = 0\n",
    "    total_rewards = 0\n",
    "    total_win_steps = 0  # Th√™m bi·∫øn ƒë·∫øm t·ªïng s·ªë b∆∞·ªõc khi th·∫Øng\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_games):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            current_player = 1\n",
    "            steps = 0  # ƒê·∫øm s·ªë b∆∞·ªõc trong m·ªói game\n",
    "            \n",
    "            while not done:\n",
    "                valid_moves = env.valid_moves()\n",
    "                if not valid_moves:\n",
    "                    break\n",
    "                \n",
    "                if current_player == 1:\n",
    "                    action = select_action(model, state, valid_moves, training=False)\n",
    "                else:\n",
    "                    action = random.choice(valid_moves)\n",
    "                \n",
    "                next_state, reward, done, _ = env.play(action)\n",
    "                steps += 1  # M·ªói l∆∞·ª£t ch∆°i l√† 1 b∆∞·ªõc\n",
    "                \n",
    "                if done:\n",
    "                    if current_player == 1:\n",
    "                        total_rewards += reward\n",
    "                        if reward == env.reward['win']:\n",
    "                            total_wins += 1\n",
    "                            total_win_steps += steps  # C·ªông s·ªë b∆∞·ªõc khi th·∫Øng\n",
    "                \n",
    "                state = next_state\n",
    "                current_player = 3 - current_player\n",
    "    \n",
    "    win_rate = total_wins / num_games\n",
    "    avg_reward = total_rewards / num_games\n",
    "    avg_win_steps = total_win_steps / total_wins if total_wins > 0 else 0  # T√≠nh trung b√¨nh\n",
    "\n",
    "    print(f\"‚úÖ Win: {total_wins} ({win_rate:.1%})\")\n",
    "    print(f\"üéØ Avg Win Steps: {avg_win_steps:.2f}\")\n",
    "    print(f\"üí∞ Avg Total Reward: {avg_reward:.2f}\")\n",
    "    return win_rate, avg_reward, avg_win_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "env = Connect4Env()\n",
    "\n",
    "input_dim = BOARD_ROW * BOARD_COL  # Size of board\n",
    "output_dim = BOARD_COL  # Ouput action (column 0 -> n)\n",
    "\n",
    "dqn_player1 = DQN(input_dim, output_dim)\n",
    "dqn_player2 = DQN(input_dim, output_dim)\n",
    "\n",
    "target_player1 = DQN(input_dim, output_dim)\n",
    "target_player1.load_state_dict(dqn_player1.state_dict())\n",
    "target_player2 = DQN(input_dim, output_dim)\n",
    "target_player2.load_state_dict(dqn_player2.state_dict())\n",
    "\n",
    "optimizer1 = optim.Adam(dqn_player1.parameters(), lr=0.001)\n",
    "optimizer2 = optim.Adam(dqn_player2.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 32\n",
    "num_episodes = 500\n",
    "epsilon = 0.1\n",
    "startTime = time.time()\n",
    "\n",
    "memory_p1 = deque(maxlen=10000)  \n",
    "memory_p2 = deque(maxlen=10000)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    turn = 1\n",
    "\n",
    "    while not done:\n",
    "        valid_moves = env.valid_moves()\n",
    "        if not valid_moves:\n",
    "            break\n",
    "            \n",
    "        # Ch·ªçn model v√† memory t∆∞∆°ng ·ª©ng\n",
    "        if turn == 1:\n",
    "            model = dqn_player1\n",
    "            target_model = target_player1\n",
    "            optimizer = optimizer1\n",
    "            memory = memory_p1\n",
    "        else:\n",
    "            model = dqn_player2\n",
    "            target_model = target_player2\n",
    "            optimizer = optimizer2\n",
    "            memory = memory_p2\n",
    "\n",
    "        action = select_action(model, state, valid_moves, episode, training=True)\n",
    "        next_state, reward, done, _ = env.play(action)\n",
    "\n",
    "        # L∆∞u experience v√†o memory\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Hu·∫•n luy·ªán\n",
    "        train_model(model, target_model, optimizer, memory, batch_size)\n",
    "\n",
    "        state = next_state\n",
    "        turn = 3 - turn\n",
    "\n",
    "    epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-episode * epsilon_decay)\n",
    "        \n",
    "    # C·∫≠p nh·∫≠t target network ƒë·ªãnh k·ª≥\n",
    "    if episode % 20 == 0:\n",
    "        target_player1.load_state_dict(dqn_player1.state_dict())\n",
    "        target_player2.load_state_dict(dqn_player2.state_dict())\n",
    "\n",
    "        win_rate, avg_reward, avg_win_steps = evaluate_model(dqn_player1, env, num_games=100)\n",
    "        all_episodes.append(episode)\n",
    "        all_win_rates.append(win_rate)\n",
    "        all_avg_rewards.append(avg_reward)\n",
    "        all_avg_win_steps.append(avg_win_steps)\n",
    "        \n",
    "        print(f\"Episode {episode}:\")\n",
    "        print(f\"  Win Rate: {win_rate:.2%}\")\n",
    "        print(f\"  Avg Reward: {avg_reward:.2f}\")\n",
    "        print(f\"  Avg Win Steps: {avg_win_steps:.2f}\")\n",
    "\n",
    "endTime = time.time()\n",
    "print(f\"Trained {num_episodes} episodes after {endTime-startTime:.4f} seconds\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dqn_player1.state_dict(), \"dqn_player1.pth\")\n",
    "torch.save(dqn_player2.state_dict(), \"dqn_player2.pth\")\n",
    "print(\"‚úÖ Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-26T12:24:43.197873Z",
     "iopub.status.idle": "2025-03-26T12:24:43.198119Z",
     "shell.execute_reply": "2025-03-26T12:24:43.198022Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# env = Connect4Env()\n",
    "\n",
    "# input_dim = BOARD_ROW * BOARD_COL  # Size of board\n",
    "# output_dim = BOARD_COL  # Ouput action (column 0 -> n)\n",
    "\n",
    "# dqn_player1 = DQN(input_dim, output_dim)\n",
    "# dqn_player2 = DQN(input_dim, output_dim)\n",
    "\n",
    "dqn_player1.load_state_dict(torch.load(\"dqn_player1.pth\"))\n",
    "dqn_player2.load_state_dict(torch.load(\"dqn_player2.pth\"))\n",
    "print(\"‚úÖ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T12:32:18.422647Z",
     "iopub.status.busy": "2025-03-26T12:32:18.422332Z",
     "iopub.status.idle": "2025-03-26T12:32:18.762955Z",
     "shell.execute_reply": "2025-03-26T12:32:18.762200Z",
     "shell.execute_reply.started": "2025-03-26T12:32:18.422618Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "evaluate_model(dqn_player1, env, num_games=100)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"‚è≥ Time taken: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_metrics(episodes, win_rates, avg_rewards, avg_win_steps):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Bi·ªÉu ƒë·ªì Win Rate\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(episodes, win_rates, 'b-o')\n",
    "    plt.title('Win Rate')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Win Rate')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Bi·ªÉu ƒë·ªì Avg Reward\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(episodes, avg_rewards, 'r-o')\n",
    "    plt.title('Avg Reward')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Bi·ªÉu ƒë·ªì Avg Win Steps\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(episodes, avg_win_steps, 'g-o')\n",
    "    plt.title('Avg Win Steps')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Steps')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_metrics.png')  # L∆∞u ·∫£nh\n",
    "    plt.show()\n",
    "\n",
    "plot_training_metrics(all_episodes, all_win_rates, all_avg_rewards, all_avg_win_steps)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
