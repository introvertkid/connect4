{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Super global constant variables\n",
    "BOARD_ROW = 6\n",
    "BOARD_COL = 7"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-26T12:30:37.350387Z",
     "iopub.status.busy": "2025-03-26T12:30:37.350105Z",
     "iopub.status.idle": "2025-03-26T12:30:37.360305Z",
     "shell.execute_reply": "2025-03-26T12:30:37.359322Z",
     "shell.execute_reply.started": "2025-03-26T12:30:37.350365Z"
    },
    "trusted": true
   },
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Connect4Env:\n",
    "    def __init__(self):\n",
    "        self.rows = BOARD_ROW\n",
    "        self.cols = BOARD_COL\n",
    "        self.board = np.zeros((self.rows, self.cols), dtype = np.int8)\n",
    "        self.current_player = 1  # Player 1 = 1, Player 2 = 2\n",
    "        self.reward = {'win': 10, 'draw': 0, 'lose': -10}\n",
    "\n",
    "    def reset(self):\n",
    "        self.__init__()\n",
    "        return self.board.flatten()\n",
    "\n",
    "    def play(self, action):\n",
    "        \"\"\"Th·ª±c hi·ªán m·ªôt h√†nh ƒë·ªông\"\"\"\n",
    "        if action not in self.valid_moves():\n",
    "            return self.board.flatten(), -10, True, {}  # K·∫øt th√∫c n·∫øu ch·ªçn sai\n",
    "\n",
    "        for row in reversed(range(self.rows)):  # T√¨m h√†ng tr·ªëng th·∫•p nh·∫•t\n",
    "            if self.board[row, action] == 0:\n",
    "                self.board[row, action] = self.current_player\n",
    "                reward, done = self.isWinningMove()\n",
    "                self.current_player = 3 - self.current_player  # ƒê·ªïi l∆∞·ª£t\n",
    "                return self.board.flatten(), reward, done, {}\n",
    "\n",
    "        return self.board.flatten(), -10, True, {}  # K·∫øt th√∫c n·∫øu c·ªôt ƒë·∫ßy\n",
    "    \n",
    "    \"\"\" Check if current state is ended after making a move\"\"\"\n",
    "    def isWinningMove(self):\n",
    "        def check_direction(r, c, dr, dc, player):\n",
    "            count = 0\n",
    "            for _ in range(4):\n",
    "                if 0 <= r < self.rows and 0 <= c < self.cols and self.board[r, c] == player:\n",
    "                    count += 1\n",
    "                    r += dr\n",
    "                    c += dc\n",
    "                else:\n",
    "                    break\n",
    "            return count == 4\n",
    "\n",
    "        for r in reversed(range(self.rows)):\n",
    "            for c in range(self.cols):\n",
    "                if self.board[r, c] != 0:\n",
    "                    player = self.board[r, c]\n",
    "                    if (check_direction(r, c, 1, 0, player) or  # Vertical\n",
    "                            check_direction(r, c, 0, 1, player) or  # Horizontal\n",
    "                            check_direction(r, c, 1, 1, player) or  # Diagonal /\n",
    "                            check_direction(r, c, 1, -1, player)):  # Diagonal \\\n",
    "                        return (self.reward['win'], True)\n",
    "                    \n",
    "        # Check draw game\n",
    "        if np.all(self.board != 0):\n",
    "            return (self.reward['draw'], True)\n",
    "        \n",
    "        return (0, False) # have not done yet\n",
    "\n",
    "    def valid_moves(self):\n",
    "        \"\"\"Tr·∫£ v·ªÅ danh s√°ch c·ªôt c√≥ th·ªÉ ƒëi\"\"\"\n",
    "        return [c for c in range(self.cols) if self.board[0, c] == 0]\n",
    "    \n",
    "    def printBoard(self):\n",
    "        for row in self.board:\n",
    "            print(\" \".join([\"‚ö´\" if x == 0 else \"üöó\" if x == 1 else \"üöï\" for x in row]))\n",
    "        print(\" 0  1   2  3   4  5   6\")\n",
    "\n",
    "board = Connect4Env()\n",
    "state = board.reset()\n",
    "print(state)\n",
    "print(board.board)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T12:30:39.835719Z",
     "iopub.status.busy": "2025-03-26T12:30:39.835427Z",
     "iopub.status.idle": "2025-03-26T12:30:39.840841Z",
     "shell.execute_reply": "2025-03-26T12:30:39.840075Z",
     "shell.execute_reply.started": "2025-03-26T12:30:39.835698Z"
    },
    "trusted": true
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        return self.layer3(x)  # Kh√¥ng d√πng softmax v√¨ DQN t·ªëi ∆∞u gi√° tr·ªã Q\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# epilson decay graph\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.1\n",
    "\n",
    "max_episode = 50\n",
    "episode = np.arange(max_episode)\n",
    "epsilon_decay = np.log(epsilon_start/epsilon_end*100) / max_episode\n",
    "\n",
    "eps = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-episode * epsilon_decay)\n",
    "plt.plot(episode, eps)\n",
    "print(epsilon_decay)\n",
    "print(eps)\n",
    "plt.title('Epsilon decay graph')\n",
    "plt.xlabel('Episode no.')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T12:37:35.326498Z",
     "iopub.status.busy": "2025-03-26T12:37:35.326186Z",
     "iopub.status.idle": "2025-03-26T12:37:35.331283Z",
     "shell.execute_reply": "2025-03-26T12:37:35.330424Z",
     "shell.execute_reply.started": "2025-03-26T12:37:35.326471Z"
    },
    "trusted": true
   },
   "source": [
    "\"\"\" Use Epsilon-greedy to find out the best way to make a move\n",
    "\n",
    "    @Params:\n",
    "    @Return: best column to play\"\"\"\n",
    "def select_action(model, state, episode=None, training=True):\n",
    "    valid_moves = env.valid_moves()\n",
    "    if not training:  \n",
    "        with torch.no_grad():\n",
    "            q_values = model(torch.tensor(state, dtype=torch.float32))\n",
    "            return valid_moves[torch.argmax(q_values[valid_moves]).item()]\n",
    "    \"\"\" Select action using epsilon-greedy with decay.\n",
    "        Calculate epsilon based on episode if training, otherwise no exploration\"\"\"\n",
    "    epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-episode * epsilon_decay) if training else 0.0\n",
    "\n",
    "    valid_moves = env.valid_moves()  # L·∫•y danh s√°ch c√°c c·ªôt h·ª£p l·ªá\n",
    "\n",
    "    if random.random() < epsilon:\n",
    "        return np.random.choice(valid_moves)  # Ch·ªçn ng·∫´u nhi√™n trong c√°c c·ªôt h·ª£p l·ªá\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = model(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
    "            return torch.argmax(q_values).item()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T12:37:37.366285Z",
     "iopub.status.busy": "2025-03-26T12:37:37.365970Z",
     "iopub.status.idle": "2025-03-26T12:37:37.370840Z",
     "shell.execute_reply": "2025-03-26T12:37:37.370188Z",
     "shell.execute_reply.started": "2025-03-26T12:37:37.366259Z"
    },
    "trusted": true
   },
   "source": [
    "\"\"\" Let 2 models play against each other\n",
    "    @Return: reward after play optimally\"\"\"\n",
    "def play_game(dqn_player1, dqn_player2, env, epsilon):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    turn = 1  # 1 l√† Player 1, 2 l√† Player 2\n",
    "\n",
    "    while not done:\n",
    "        model = dqn_player1 if turn == 1 else dqn_player2\n",
    "        action = select_action(model, state, epsilon)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            return reward  # Tr·∫£ v·ªÅ k·∫øt qu·∫£ tr·∫≠n ƒë·∫•u\n",
    "\n",
    "        state = next_state\n",
    "        turn = 3 - turn  # ƒê·ªïi l∆∞·ª£t ch∆°i\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T12:37:39.454302Z",
     "iopub.status.busy": "2025-03-26T12:37:39.454026Z",
     "iopub.status.idle": "2025-03-26T12:37:39.459829Z",
     "shell.execute_reply": "2025-03-26T12:37:39.459003Z",
     "shell.execute_reply.started": "2025-03-26T12:37:39.454281Z"
    },
    "trusted": true
   },
   "source": [
    "from collections import deque\n",
    "\n",
    "\"\"\"@Params: \"\"\"\n",
    "def train_model(model, optimizer, memory, batch_size):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    \n",
    "    for state, action, reward, next_state, done in batch:\n",
    "        q_values = model(torch.tensor(state, dtype=torch.float32))\n",
    "        q_value = q_values[action]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_q_values = model(torch.tensor(next_state, dtype=torch.float32))\n",
    "            target_q_value = reward if done else reward + 0.99 * torch.max(next_q_values)\n",
    "\n",
    "        loss = (q_value - target_q_value) ** 2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "env = Connect4Env()\n",
    "\n",
    "input_dim = BOARD_ROW * BOARD_COL  # Size of board\n",
    "output_dim = BOARD_COL  # Ouput action (column 0 -> n)\n",
    "\n",
    "dqn_player1 = DQN(input_dim, output_dim)\n",
    "dqn_player2 = DQN(input_dim, output_dim)\n",
    "\n",
    "optimizer1 = optim.Adam(dqn_player1.parameters(), lr=0.001)\n",
    "optimizer2 = optim.Adam(dqn_player2.parameters(), lr=0.001)\n",
    "\n",
    "memory = deque(maxlen=10000)\n",
    "batch_size = 64\n",
    "num_episodes = 10\n",
    "epsilon = 0.1\n",
    "startTime = time.time()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    turn = 1  # 1 l√† Player 1, 2 l√† Player 2\n",
    "\n",
    "    while not done:\n",
    "        model = dqn_player1 if turn == 1 else dqn_player2\n",
    "        optimizer = optimizer1 if turn == 1 else optimizer2\n",
    "\n",
    "        valid_moves = env.valid_moves()\n",
    "        if not valid_moves:\n",
    "            break  \n",
    "\n",
    "        valid_moves = env.valid_moves()\n",
    "        if not valid_moves:\n",
    "            break  \n",
    "\n",
    "        action = select_action(model, state, episode)\n",
    "        next_state, reward, done, _ = env.play(action)\n",
    "\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        train_model(model, optimizer, memory, batch_size)\n",
    "\n",
    "        state = next_state\n",
    "        turn = 3 - turn  # ƒê·ªïi l∆∞·ª£t ch∆°i\n",
    "\n",
    "    epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-episode * epsilon_decay)\n",
    "\n",
    "    # ƒê√°nh gi√° v√† in th√¥ng tin m·ªói 10 episode\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}: Reward {reward}, Epsilon {epsilon:.4f}\")\n",
    "        print(f\"Training Stats - P1 Wins: {player1_wins}, P2 Wins: {player2_wins}, Draws: {draws}\")\n",
    "        win_rate, avg_win_steps, avg_total_reward = evaluate_model(dqn_player1, env, num_games=10)\n",
    "        print(f\"Evaluate at Episode {episode}: Win Rate {win_rate:.2%}, Avg Win Steps {avg_win_steps:.2f}, Avg Reward {avg_total_reward:.2f}\")\n",
    "\n",
    "endTime = time.time()\n",
    "print(f\"Trained {num_episodes} episodes after {endTime-startTime:.4f} seconds\")\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, env, num_games=100):\n",
    "    total_wins = 0\n",
    "    total_losses = 0\n",
    "    total_draws = 0\n",
    "    total_win_steps = 0\n",
    "    total_rewards = 0\n",
    "\n",
    "    for _ in range(num_games):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        current_player = 1  # Model l√† Player 1\n",
    "        steps = 0\n",
    "        winner = None\n",
    "        final_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            steps += 1\n",
    "            valid_moves = env.valid_moves()\n",
    "            \n",
    "            # Ch·ªçn h√†nh ƒë·ªông\n",
    "            if current_player == 1:\n",
    "                action = select_action(model, state, training=False)\n",
    "            else:\n",
    "                action = random.choice(valid_moves)  # ƒê·ªëi th·ªß ng·∫´u nhi√™n\n",
    "            \n",
    "            # Th·ª±c hi·ªán h√†nh ƒë·ªông\n",
    "            next_state, reward, done, _ = env.play(action)\n",
    "            \n",
    "            # Ghi l·∫°i th√¥ng tin khi k·∫øt th√∫c game\n",
    "            if done:\n",
    "                final_reward = reward\n",
    "                last_player = current_player\n",
    "            \n",
    "            state = next_state\n",
    "            current_player = 3 - current_player  # ƒê·ªïi l∆∞·ª£t\n",
    "\n",
    "        # X√°c ƒë·ªãnh k·∫øt qu·∫£ t·ª´ g√≥c nh√¨n c·ªßa model\n",
    "        if final_reward == env.reward['win']:\n",
    "            winner = last_player\n",
    "        elif final_reward == env.reward['draw']:\n",
    "            winner = None\n",
    "        else:\n",
    "            winner = None  # Tr∆∞·ªùng h·ª£p kh√¥ng h·ª£p l·ªá (kh√¥ng x·∫£y ra khi ƒë√°nh gi√°)\n",
    "\n",
    "        # C·∫≠p nh·∫≠t th·ªëng k√™\n",
    "        if winner == 1:\n",
    "            total_wins += 1\n",
    "            total_win_steps += steps\n",
    "            total_rewards += env.reward['win']\n",
    "        elif winner == 2:\n",
    "            total_losses += 1\n",
    "            total_rewards += env.reward['lose']\n",
    "        else:\n",
    "            total_draws += 1\n",
    "            total_rewards += env.reward['draw']\n",
    "\n",
    "    # T√≠nh to√°n c√°c ch·ªâ s·ªë\n",
    "    win_rate = total_wins / num_games\n",
    "    loss_rate = total_losses / num_games\n",
    "    draw_rate = total_draws / num_games\n",
    "    avg_win_steps = total_win_steps / total_wins if total_wins > 0 else 0\n",
    "    avg_total_reward = total_rewards / num_games\n",
    "\n",
    "    # In k·∫øt qu·∫£\n",
    "    print(f\"üìä Evaluate on {num_games} game:\")\n",
    "    print(f\"‚úÖ Win: {total_wins} ({win_rate:.1%})\")\n",
    "    print(f\"‚ùå Lose: {total_losses} ({loss_rate:.1%})\")\n",
    "    print(f\"ü§ù Draw: {total_draws} ({draw_rate:.1%})\")\n",
    "    print(f\"üéØ Avg Win Steps: {avg_win_steps:.2f}\")\n",
    "    print(f\"üí∞ Avg Total Reward: {avg_total_reward:.2f}\")\n",
    "\n",
    "    return win_rate, avg_win_steps, avg_total_reward"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.save(dqn_player1.state_dict(), \"dqn_player1.pth\")\n",
    "torch.save(dqn_player2.state_dict(), \"dqn_player2.pth\")\n",
    "print(\"‚úÖ Model saved successfully!\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-26T12:24:43.197873Z",
     "iopub.status.idle": "2025-03-26T12:24:43.198119Z",
     "shell.execute_reply": "2025-03-26T12:24:43.198022Z"
    },
    "trusted": true
   },
   "source": [
    "# import time\n",
    "\n",
    "# env = Connect4Env()\n",
    "\n",
    "# input_dim = BOARD_ROW * BOARD_COL  # Size of board\n",
    "# output_dim = BOARD_COL  # Ouput action (column 0 -> n)\n",
    "\n",
    "# dqn_player1 = DQN(input_dim, output_dim)\n",
    "# dqn_player2 = DQN(input_dim, output_dim)\n",
    "\n",
    "dqn_player1.load_state_dict(torch.load(\"dqn_player1.pth\"))\n",
    "dqn_player2.load_state_dict(torch.load(\"dqn_player2.pth\"))\n",
    "print(\"‚úÖ Model loaded successfully!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T12:32:18.422647Z",
     "iopub.status.busy": "2025-03-26T12:32:18.422332Z",
     "iopub.status.idle": "2025-03-26T12:32:18.762955Z",
     "shell.execute_reply": "2025-03-26T12:32:18.762200Z",
     "shell.execute_reply.started": "2025-03-26T12:32:18.422618Z"
    },
    "trusted": true
   },
   "source": [
    "start_time = time.time()\n",
    "evaluate_model(dqn_player1, env, num_games=100)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"‚è≥ Time taken: {end_time - start_time:.2f} seconds\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
